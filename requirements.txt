torch>=1.12.0
transformers>=4.20.0
datasets>=2.0.0
accelerate>=0.12.0
tokenizers>=0.12.0
numpy>=1.21.0
tqdm>=4.62.0
sentencepiece
protobuf
peft>=0.4.0
bitsandbytes>=0.39.0  # For 8-bit optimization (optional)
scipy>=1.7.0